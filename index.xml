<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title/><link>https://hyeamykim.github.io/</link><description>Recent content on</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 06 Mar 2025 11:29:24 +0100</lastBuildDate><atom:link href="https://hyeamykim.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Set Up Macbook for Ml</title><link>https://hyeamykim.github.io/post/how-to-set-up-macbook-for-ml/</link><pubDate>Thu, 06 Mar 2025 11:29:24 +0100</pubDate><guid>https://hyeamykim.github.io/post/how-to-set-up-macbook-for-ml/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>It was my first time setting up and using MacBook (MacBook Air M1, Mac OS Sequoia 15.3) to use it for data analytics, ML, and LLM projects. Though I found helpful YouTube tutorials, it was still quite a process, so I thought I would share it in case anyone needs it as well.&lt;/p>
&lt;p>I followed these YouTube tutorials by Danil Zherebtsov:&lt;br>
(You can watch them in order if it&amp;rsquo;s easier for you to process information by watching instead of reading.)&lt;/p></description></item><item><title>How to Measure Bias in Word Embeddings</title><link>https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/</link><pubDate>Mon, 02 Dec 2024 16:51:15 +0100</pubDate><guid>https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>It has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016).
Bias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing.&lt;/p></description></item></channel></rss>