<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><meta charset=utf-8><title>hyeamykim
|
How to Measure Bias in Word Embeddings
</title><meta name=generator content="Hugo 0.139.2"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="hyeamykim"><meta name=description content="Showcasing My Project-based-learning"><link rel=stylesheet href=/scss/main.min.8d4fad7e6916ad2e291e8d97ada157c70350d6d7150fea137e7243340967befb.css integrity="sha256-jU+tfmkWrS4pHo2XraFXxwNQ1tcVD+oTfnJDNAlnvvs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC+3978NBRk=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css integrity="sha256-5l3FtI+185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR+rP2S8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css integrity="sha256-4QQlrXaLyY/x+ycqCshCD50boi8GEsCP8QEMlQgP/n4=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script><script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Measure Bias in Word Embeddings"><meta name=twitter:description content="Introduction It has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016). Bias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing."><meta property="og:url" content="https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/"><meta property="og:site_name" content="My blog"><meta property="og:title" content="How to Measure Bias in Word Embeddings"><meta property="og:description" content="Introduction It has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016). Bias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-12-02T16:51:15+01:00"><meta property="article:modified_time" content="2024-12-02T16:51:15+01:00"><meta property="article:tag" content="Word Embeddings"><meta property="article:tag" content="Social Bias"><meta property="article:tag" content="Debiasing"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"How to Measure Bias in Word Embeddings","headline":"How to Measure Bias in Word Embeddings","alternativeHeadline":"","description":"
      
        \u003ch2 id=\u0022introduction\u0022\u003eIntroduction\u003c\/h2\u003e\n\u003cp\u003eIt has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016).\nBias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing.\u003c\/p\u003e


      


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/hyeamykim.github.io\/post\/how-to-measure-bias-in-word-embeddings\/"},"author":{"@type":"Person","name":"hyeamykim"},"creator":{"@type":"Person","name":"hyeamykim"},"accountablePerson":{"@type":"Person","name":"hyeamykim"},"copyrightHolder":{"@type":"Person","name":"hyeamykim"},"copyrightYear":"2024","dateCreated":"2024-12-02T16:51:15.00Z","datePublished":"2024-12-02T16:51:15.00Z","dateModified":"2024-12-02T16:51:15.00Z","publisher":{"@type":"Organization","name":"hyeamykim","url":"https://hyeamykim.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/hyeamykim.github.io\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/hyeamykim.github.io\/post\/how-to-measure-bias-in-word-embeddings\/","wordCount":"1609","genre":[],"keywords":["word embeddings","social bias","debiasing"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/profile_photo.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>My blog</a></div><div class=sidebar__introduction-description><p>Showcasing My Project-based-learning</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/hye-yeon-kim-60222090/ target=_blank rel="noopener me" aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://medium.com/@hyeamykim target=_blank rel="noopener me" aria-label=Medium title=Medium><i class="fab fa-medium fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
hyeamykim
2024</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/about/ title>About</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>How to Measure Bias in Word Embeddings</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>Mon, Dec 2, 2024</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>8-minute read</span></li></ul><h2 id=introduction>Introduction</h2><p>It has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016).
Bias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing.</p><h3 id=disclaimer>Disclaimer</h3><ul><li>This post covers static word embeddings, like Word2Vec or GloVe, that provide a fixed mapping between words and vector representations, regardless of the context.</li><li>The metrics explained here came from literature covering debiasing gender bias in word embeddings, but they could be used for detecting other types of social biases.</li><li>The metrics and datasets mentioned here are not comprehensive and are just a quick introduction to gender bias measurement in static word embeddings.</li><li>This post was first published on <a href=https://medium.com/@hyeamykim/how-to-measure-bias-in-word-embeddings-e48c705221db>Medium</a></li></ul><h2 id=bias-mitigation>Bias Mitigation</h2><p>Many debiasing techniques are based on geometric projection in the embedding space and use the cosine distances between words for detecting and removing bias. This indicates that bias is associated with geometric asymmetry between the words in the embedding space. (James and Alvarez-Melis, 2020) Specifically, words projected onto a gender subspace, which can be defined from a set of predetermined gender-definitional word pairs such as he-she, man-woman, king-queen, etc., are used to study the asymmetry between some target words and gender-definitional words.</p><h2 id=bias-measurement-methods>Bias Measurement Methods</h2><h3 id=word-embedding-association-test-weat-caliskan-et-al-2017>Word Embedding Association Test (WEAT) (Caliskan et al., 2017)</h3><p>WEAT quantifies biases regarding gender, race, age, etc., using semantic similarities between word embeddings.</p><p>The score is calculated by comparing cosine similarity values between sets of target words X and Y (e.g., European and African names) with two sets of attribute words A and B (e.g., pleasant vs. unpleasant), and is defined as follows:</p>$$s(X, Y, A, \mathcal{B}) = \sum_{x \in X} k(x, A, \mathcal{B}) - \sum_{y \in Y} k(y, A, \mathcal{B})$$$$k(t, A, \mathcal{B}) = \text{mean}_{a \in A} f(t, a) - \text{mean}_{b \in \mathcal{B}} f(t, b)$$<p>Where f is cosine similarity.</p><p>For all words x in a target group X, the mean value of cosine similarity values between x and all words a in an attribute group A and the mean value of cosine similarity values between x and all words b in an attribute group B are calculated. The difference between the cosine similarity values is then calculated. The sum of all the differences for all words in a target group X is the measure that indicates whether target group X words are more similar to — more associated with — attribute group A or B on average. The same values are calculated for all words y belonging to a target group Y. Then, the difference between the measure for target groups X and Y is the final score for indicating bias.</p><p>The score ranges between -2 and 2, and when the score is close to zero, we can say that neither target is more strongly associated with a certain attribute. A high positive score tells us target X is more strongly associated with attribute A, and a high negative score indicates target Y is more strongly associated with attribute A.</p><h3 id=word-association-test-wat-du-et-al-2019>Word Association Test (WAT) (Du et al., 2019)</h3><p>In WAT, the gender information in a word association graph created with Small World of Words (SWOWEN) is calculated to get a sense of gender bias over a large vocabulary (Deyene et al., 2019).</p><p>The gender information vector for each word is represented as a 2-dimensional vector that contains the masculine and feminine orientations of a word. A set of gender-specific word pairs L, which consists of masculine and feminine words, is pre-defined.</p><p>P is the matrix containing vectors of all words. For the gender-specific words in L, the vector for masculine words is initialized with (1, 0) and feminine words with (0, 1). The vector for other words is initialized with (0, 0). Then, the gender information vector is propagated using a random walk until convergence</p>$$P_{t+1} = \alpha T P_t + (1 - \alpha) P_0,$$$$T = D^{-\frac{1}{2}} S D^{-\frac{1}{2}}, \quad D = \text{diag}_i \left( \sum_{j=1}^N S_{ij} \right)$$<p>where T is the normalized adjacency matrix of the word association graph, S is the adjacency matrix, and α is a hyperparameter in range (0, 1) which balances global and local consistency. Small α indicates higher global consistency which values keeping the initial vectors of words and large α means higher local consistency, which values the smoothness between connected words (i.e. close words in a graph should have similar labels).</p><p>After the propagation, the bias score of a word is calculated as</p>$$b = \log \frac{\overline{b}_m}{\overline{b}_f}$$<h3 id=neighborhood-metric-gonen-and-goldberg-2019>Neighborhood metric (Gonen and Goldberg, 2019)</h3><p>The neighborhood bias metric quantifies bias as the percentage of male socially biased words among the k nearest socially biased male- and female-neighboring words in the embedding space, where words are projected onto a gender subspace.</p><p>It was proposed by Gonen and Goldberg (2019), based on the idea that bias still can be observed with the clustering of socially-marked gendered words. For example, even after debiasing, the word “nurse” might not be close to gender-definitional feminine words in the embedding space, but might still be close to other socially-biased female words such as “receptionist,” “caregiver,” and “teacher.”</p><p>In James et al. (2020), the 1,000 most socially-biased words (500 male and 500 female) in the vocabulary were used and a word’s bias was measured as the ratio of its neighborhood of socially-biased male and socially-biased female words. So a value of 0.5 in this metric would indicate a perfectly unbiased word, and values closer to 0 and 1 indicates stronger bias.</p><h3 id=relational-inner-product-association-ripa-ethayarajh-et-al-2019>Relational Inner Product Association (RIPA) (Ethayarajh et al, 2019)</h3><p>RIPA was suggested as an alternative to WEAT. It quantifies bias as the inner product association of a word vector v with respect to a relation vector b, which is constructed from the first principal component of the differences between gender word pairs. In other words, RIPA is the scalar projection of a word vector onto a one-dimensional bias subspace defined by the unit vector b.</p><p>RIPA is an intuitive measure, which allows us to directly compare the actual word association in embedding space with what we would expect the word association to be, given the training corpus.</p><h3 id=winobias-zhao-et-al-2018a>WinoBias (Zhao et al., 2018a)</h3><p>The WinoBias dataset was devised to evaluate coreference resolution tasks and contains winograd-schema-style sentences with entities corresponding to people referred to by their occupations. It consists of two types of sentences that link gendered pronouns to either male or female stereotypical occupations.</p><p>Type 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances].</p><p>Type 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances].</p><p>In Type 1, co-reference decisions must be made using world knowledge about some given circumstances, and in Type 2, co-reference can be resolved with syntactic information and an understanding of the pronouns.</p><p>The dataset can be also categorized by pro-stereotyped sentences, which link pronouns to occupations dominated by the gender of the pronoun, and anti-stereotyped sentences, which link pronouns to occupations not dominated by the gender of the pronoun.</p><p>The bias in word embeddings can be measured by comparing the co-reference performance for pro-stereotyped and anti-stereotyped sentences both Type 1 and Type 2. Ideally, the co-reference decision should be made with the same accuracy.</p><h3 id=sembias-zhao-et-al-2018b>SemBias (Zhao et al., 2018b)</h3><p>SemBias dataset was devised by Zhao et al. (2018b), inspired by SemEval2012 Task2, to study gender information in word embeddings. It contains three types of word pairs: (1) Definition, which are gender-definition word pairs (e.g., hero — heroine), (2) Stereotype, which are gender stereotype word pairs (e.g., manager — secretary), and (3) None, which are two other word-pairs (e.g., jazz — blues, pencil — pen). The designed goal is to identify the correct analogy he-she from these four pairs of words.</p><p>With the SemBias dataset, bias in word embeddings can be evaluated by comparing word embedding’s performance on identifying he-she analogy from the definition set and from the stereotype set, for example. Another way is to calculate the cosine similarity between the gender directional vector and the definition set and the stereotype set, respectively (Kaneko and Bollegala, 2021).</p><h2 id=references>References</h2><ul><li>Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings, Bolukbasi et al., 2016, <a href=https://arxiv.org/pdf/1607.06520.pdf>https://arxiv.org/pdf/1607.06520.pdf</a></li><li>Semantics derived automatically from language corpora contain human-like biases, Caliskan et al., 2017, <a href=https://arxiv.org/pdf/1608.07187.pdf>https://arxiv.org/pdf/1608.07187.pdf</a></li><li>Gender bias in coreference resolution: Evaluation and debiasing methods, Zhao et al., 2018a, <a href=https://arxiv.org/pdf/1804.06876.pdf>https://arxiv.org/pdf/1804.06876.pdf</a></li><li>Learning Gender-Neutral Word Embeddings, Zhao et al., 2018b, <a href=https://arxiv.org/pdf/1809.01496.pdf>https://arxiv.org/pdf/1809.01496.pdf</a>
Understanding Undesirable Word Embedding Associations, Ethayarajh et al., 2019, <a href=https://arxiv.org/pdf/1908.06361.pdf>https://arxiv.org/pdf/1908.06361.pdf</a></li><li>Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them, Gonen and Goldberg, 2019, <a href=https://arxiv.org/pdf/1903.03862.pdf>https://arxiv.org/pdf/1903.03862.pdf</a></li><li>Exploring Human Gender Stereotypes with Word Association Test, Du et al., 2019, <a href=https://aclanthology.org/D19-1635.pdf>https://aclanthology.org/D19-1635.pdf</a></li><li>The “Small World of Words” English word association norms for over 12,000 cue words, Deyne et al., 2019, <a href=https://link.springer.com/article/10.3758/s13428-018-1115-7>https://link.springer.com/article/10.3758/s13428-018-1115-7</a></li><li>Probabilistic Bias Mitigation in Word Embeddings, James and Alvarez-Melis, 2020, <a href=https://arxiv.org/pdf/1910.14497.pdf>https://arxiv.org/pdf/1910.14497.pdf</a></li><li>Dictionary-based Debiasing of Pre-trained Word Embeddings, Kaneko and Bollegala, 2021, <a href=https://arxiv.org/pdf/2101.09525.pdf>https://arxiv.org/pdf/2101.09525.pdf</a></li><li>Bias and Fairness in Large Language Models: A Survey, Gallegos et al., 2023, <a href=https://arxiv.org/pdf/2309.00770>https://arxiv.org/pdf/2309.00770</a></li></ul></div><div class=post__footer><span><a class=tag href=/tags/word-embeddings/>word embeddings</a><a class=tag href=/tags/social-bias/>social bias</a><a class=tag href=/tags/debiasing/>debiasing</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
hyeamykim
2024</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>