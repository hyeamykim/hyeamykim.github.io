<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on</title><link>https://hyeamykim.github.io/post/</link><description>Recent content in Posts on</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 02 Dec 2024 16:51:15 +0100</lastBuildDate><atom:link href="https://hyeamykim.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Measure Bias in Word Embeddings</title><link>https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/</link><pubDate>Mon, 02 Dec 2024 16:51:15 +0100</pubDate><guid>https://hyeamykim.github.io/post/how-to-measure-bias-in-word-embeddings/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>It has been well-known that word embeddings trained with human-written corpus reflect harmful biases (Bolukbasi et al., 2016).
Bias os social bias, in general, can be understood as an oversimplified view or prejudiced attitude towards a particular type of person. In the context of NLP, social bias includes disparate treatment or outcomes between different social groups (Gallegos et al., 2023). For example, stronger associations of certain groups of people (e.g., women) to certain attributes (e.g., domestic professions such as homemakers) that indicate such oversimplified views can be found in word embeddings. There have been many studies to inspect and mitigate bias in word embeddings since such bias can be propagated in various downstream tasks that use word embeddings as features and harm minorities in the decision-making process. In this post, various methods and datasets devised to measure such bias in word embeddings will be explained. These are mostly used to evaluate debiasing methods by comparing the bias metric before and after debiasing.&lt;/p></description></item></channel></rss>